# -*- coding: utf-8 -*-
"""Book Recommender System f.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1roNcdlHZQOVxcqYlegicITjSw5EofBKo
"""

# Cell 1: Config and imports
import pandas as pd
import numpy as np
import scipy.sparse as sps
from sklearn.neighbors import NearestNeighbors
import gc
from typing import List

# --- User-tunable params ---
# RATING_THRESHOLD = 5          # keep ratings strictly greater than this
# MIN_USER_RATINGS = 20        # user must have at least this many ratings to be "frequent"
# MIN_BOOK_RATINGS = 5         # optionally keep books with >= this many retained ratings
# TOP_K = 5                    # number of recommendations to return (including query)
# --- Load configuration parameters from YAML ---
import yaml

with open("config/params.yaml", "r") as f:
    params = yaml.safe_load(f)

RATING_THRESHOLD = params["RATING_THRESHOLD"]
MIN_USER_RATINGS = params["MIN_USER_RATINGS"]
MIN_BOOK_RATINGS = params["MIN_BOOK_RATINGS"]
TOP_K = params["TOP_K"]

print(f"‚úÖ Loaded parameters from YAML: "
      f"RATING_THRESHOLD={RATING_THRESHOLD}, "
      f"MIN_USER_RATINGS={MIN_USER_RATINGS}, "
      f"MIN_BOOK_RATINGS={MIN_BOOK_RATINGS}, "
      f"TOP_K={TOP_K}")

# Helpful: reduce pandas warnings / show shapes
pd.options.mode.chained_assignment = None

# Cell 2: Read only needed columns with usecols and small dtypes
# Make sure the CSV files are in Colab (or uploaded) as Books.csv, Ratings.csv, Users.csv

books_usecols = ['ISBN', 'Book-Title', 'Book-Author', 'Publisher', 'Year-Of-Publication']
ratings_usecols = ['User-ID', 'ISBN', 'Book-Rating']
users_usecols = ['User-ID', 'Location', 'Age']

# If your CSVs have these exact headers this will work. If not, the later normalization cell will help.
books = pd.read_csv('Books.csv', usecols=lambda c: c in books_usecols, encoding='latin-1', low_memory=True)
ratings = pd.read_csv('Ratings.csv', usecols=lambda c: c in ratings_usecols, encoding='latin-1', low_memory=True)
users = pd.read_csv('Users.csv', usecols=lambda c: c in users_usecols, encoding='latin-1', low_memory=True)

print("Loaded shapes -> books:", books.shape, "ratings:", ratings.shape, "users:", users.shape)

# Cell 3: Normalize column names to lowercase/underscores and cast types to save memory
def normalize_cols(df):
    df = df.copy()
    df.columns = [c.strip().lower().replace(' ', '_') for c in df.columns]
    return df

books = normalize_cols(books)
ratings = normalize_cols(ratings)
users = normalize_cols(users)

# canonical names expected by later cells
# books: isbn, book-title, book-author, publisher, year-of-publication
# ratings: user-id, isbn, book-rating
# users: user-id, location, age

# cast types: user-id and isbn to string, rating numeric
ratings['user-id'] = ratings['user-id'].astype(str)
ratings['isbn'] = ratings['isbn'].astype(str)
ratings['book-rating'] = pd.to_numeric(ratings['book-rating'], errors='coerce')

books['isbn'] = books['isbn'].astype(str)
if 'year-of-publication' in books.columns:
    books['year-of-publication'] = pd.to_numeric(books['year-of-publication'], errors='coerce').astype('Int32')

if 'user-id' in users.columns:
    users['user-id'] = users['user-id'].astype(str)
    users['age'] = pd.to_numeric(users.get('age'), errors='coerce').astype('Int32')

# quick preview
print("Columns after normalize:")
print("books:", books.columns.tolist())
print("ratings:", ratings.columns.tolist())
print("users:", users.columns.tolist())

# Cell 4: Filter by rating_threshold immediately to drop many rows early
print("Initial ratings shape:", ratings.shape)

ratings = ratings[ratings['book-rating'] > RATING_THRESHOLD].copy()
ratings = ratings.dropna(subset=['user-id', 'isbn']).reset_index(drop=True)

print(f"After keeping ratings > {RATING_THRESHOLD}: {ratings.shape}")

# Cell 5: Keep only frequent users (reduce noise & memory)
user_counts = ratings['user-id'].value_counts()
active_users = user_counts[user_counts >= MIN_USER_RATINGS].index
ratings = ratings[ratings['user-id'].isin(active_users)].reset_index(drop=True)
print(f"After keeping users with >= {MIN_USER_RATINGS} ratings: {ratings.shape}")

# Optional: drop books that now have too few retained ratings
book_counts_after = ratings['isbn'].value_counts()
popular_books = book_counts_after[book_counts_after >= MIN_BOOK_RATINGS].index
ratings = ratings[ratings['isbn'].isin(popular_books)].reset_index(drop=True)
print(f"After keeping books with >= {MIN_BOOK_RATINGS} retained ratings: {ratings.shape}")

# Cell 6: Merge ratings with books (inner join) - keep only columns needed
books_small_cols = ['isbn', 'book-title', 'book-author', 'publisher']
available = [c for c in books_small_cols if c in books.columns]
books_small = books[available].drop_duplicates(subset=['isbn']).set_index('isbn')

# Left join ratings with book metadata (ratings filtered earlier)
merged = ratings.merge(books_small.reset_index(), on='isbn', how='inner')
print("Merged shape (ratings + books):", merged.shape)

# Merge with user info if present (optional, small)
if 'user-id' in users.columns:
    users_small = users[['user-id', 'location', 'age']].drop_duplicates(subset=['user-id'])
    merged = merged.merge(users_small, on='user-id', how='left')
    print("Merged with users shape:", merged.shape)

# Keep only the essential columns for recommender
keep_cols = ['user-id', 'isbn', 'book-rating', 'book-title', 'book-author']
keep_cols = [c for c in keep_cols if c in merged.columns]
merged = merged[keep_cols].drop_duplicates().reset_index(drop=True)
print("Final merged shape (kept cols):", merged.shape)
merged.head(3)

# Cell 7: helper to free memory as we go
def drop_and_gc(*dfs_or_names):
    """
    Pass DataFrames or names (strings) to drop from globals, or delete DataFrames.
    Example: drop_and_gc('books', 'users') will del globals()['books'] and run gc.
    """
    g = globals()
    for x in dfs_or_names:
        if isinstance(x, str):
            if x in g:
                del g[x]
        else:
            # try to delete the object reference
            try:
                del x
            except Exception:
                pass
    gc.collect()

# Example: free original large dataframes if no longer needed
drop_and_gc('books', 'ratings', 'users')
print("Freed books, ratings, users from memory (if present).")

# Cell 8: Create sparse matrix (books x users) using small integer indices
# Map each book title to an index, each user to an index
book_titles = merged['book-title'].unique()
book_to_idx = {b: i for i, b in enumerate(book_titles)}
user_ids = merged['user-id'].unique()
user_to_idx = {u: i for i, u in enumerate(user_ids)}

n_books = len(book_titles)
n_users = len(user_ids)
print("Number of unique books:", n_books, "Number of unique users:", n_users)

# Build sparse matrix: rows = books, cols = users
rows = merged['book-title'].map(book_to_idx).astype(np.int32)
cols = merged['user-id'].map(user_to_idx).astype(np.int32)
data = merged['book-rating'].astype(np.float32)

# Create CSR matrix (books x users)
R = sps.csr_matrix((data.values, (rows.values, cols.values)), shape=(n_books, n_users))
print("Sparse rating matrix shape:", R.shape, "nnz:", R.nnz)

# Cell 9: Fit NearestNeighbors on item vectors (each row = a book)
# Use cosine metric. n_neighbors should be a bit larger than TOP_K to allow filtering.
nn_model = NearestNeighbors(metric='cosine', algorithm='brute', n_jobs=-1)
nn_model.fit(R)   # R is sparse csr (books x users)
print("NearestNeighbors model fitted on sparse item vectors.")

# Cell 10: Function to recommend top-K similar books given a book title
def recommend_books_sparse(book_name: str, top_k: int = TOP_K) -> List[str]:
    """
    Returns up to top_k book titles most similar to book_name (including the book itself).
    Uses the fitted NearestNeighbors model on sparse item vectors.
    """
    if book_name not in book_to_idx:
        print(f"‚ùå Book '{book_name}' not found in dataset. Try exact title from merged['book-title'].unique().")
        return []
    idx = book_to_idx[book_name]
    # query - returns distances and indices
    distances, indices = nn_model.kneighbors(R[idx], n_neighbors=top_k)
    distances = distances.flatten()
    indices = indices.flatten()
    # convert indices to titles and show similarity (1 - cosine_distance)
    results = []
    for i, d in zip(indices, distances):
        title = book_titles[i]
        sim = 1 - d  # similarity
        results.append((title, sim))
    print(f"\nRecommendations for '{book_name}':")
    for rank, (t, s) in enumerate(results, 1):
        print(f"{rank}. {t}  (sim={s:.3f})")
    return [t for t, _ in results]

# Example: replace with a real title from your data
# recommend_books_sparse("The Hobbit", top_k=5)

# Cell 11: Save the processed merged small dataframe and/or sparse matrix metadata
merged.to_csv('merged_filtered_small.csv', index=False)
# Save mapping dicts using pickle if you want to reload later
import pickle
with open('mappings.pkl', 'wb') as f:
    pickle.dump({'book_to_idx': book_to_idx, 'user_to_idx': user_to_idx, 'book_titles': list(book_titles)}, f)

# Save sparse matrix in efficient format
sps.save_npz('R_books_users.npz', R)

print("Saved merged_filtered_small.csv, mappings.pkl, and R_books_users.npz")

# Cell 12: Interactive test for your recommender system

# Ensure the model and mappings are available
try:
    _ = nn_model
except NameError:
    print("‚ö†Ô∏è Model not found. Please run Cells 8‚Äì10 first.")
else:
    # Ask for a book title
    book_name = input("üîç Enter a book title to get recommendations: ").strip()

    # Check and recommend
    if book_name:
        recommended = recommend_books_sparse(book_name, top_k=TOP_K)
        if recommended:
            print("\n‚úÖ Top recommendations:")
            for i, title in enumerate(recommended, 1):
                print(f"{i}. {title}")
    else:
        print("‚ùå No title entered.")

# Cell A: show how titles look (helps to copy exact titles)
try:
    sample_titles = list(book_titles[:])  # first 200 titles
except NameError:
    print("Model variables not found. Run the model-building cells (Cells 8-10).")
else:
    print("Sample book titles (first 200). Use these exact strings or try the interactive helper below:")
    for i, t in enumerate(sample_titles[:50], 1):  # show first 50 for brevity
        print(f"{i}. {t}")
    print("\n(There are total {} unique titles)".format(len(book_titles)))

# Cell B: Build normalization and matching helpers
import re
from difflib import get_close_matches

def normalize_title(s: str) -> str:
    """Normalize a title for matching: lower, strip, remove punctuation, collapse spaces."""
    if not isinstance(s, str):
        return ""
    s = s.strip().lower()
    # remove punctuation except alphanumerics and spaces
    s = re.sub(r'[^0-9a-zA-Z\s]', '', s)
    s = re.sub(r'\s+', ' ', s)
    return s

# Build mapping: normalized -> set of canonical titles (to handle duplicates)
norm_to_titles = {}
for t in book_titles:
    n = normalize_title(t)
    norm_to_titles.setdefault(n, []).append(t)

# Also keep a flat list of normalized titles for fuzzy matching
norm_titles_list = list(norm_to_titles.keys())

print("Built normalized mapping. Example normalized -> canonical mapping (first 10):")
for k in list(norm_to_titles.keys())[:10]:
    print(k, "->", norm_to_titles[k][:3])

# Cell C: Resolve user input (exact/substring/fuzzy). Returns list of candidate canonical titles.
def resolve_title_input(user_input: str, n_candidates: int = 8):
    s = normalize_title(user_input)
    if not s:
        return []
    # 1) exact normalized match
    if s in norm_to_titles:
        return norm_to_titles[s]
    # 2) substring match on normalized titles (fast)
    substr_matches = [norm_to_titles[norm] for norm in norm_titles_list if s in norm]
    if substr_matches:
        # flatten and return unique canonical titles (limit)
        flat = []
        for group in substr_matches:
            for x in group:
                if x not in flat:
                    flat.append(x)
                    if len(flat) >= n_candidates:
                        return flat
        return flat
    # 3) fuzzy (difflib) on normalized titles
    close = get_close_matches(s, norm_titles_list, n=n_candidates, cutoff=0.6)
    if close:
        flat = []
        for norm in close:
            for x in norm_to_titles[norm]:
                if x not in flat:
                    flat.append(x)
                    if len(flat) >= n_candidates:
                        return flat
        return flat
    # no match
    return []

# Cell D: Interactive text input + dropdown fallback. Works in notebook.
try:
    import ipywidgets as widgets
    from IPython.display import display, clear_output
    use_widgets = True
except Exception:
    use_widgets = False

def interactive_recommend():
    # Step 1: ask for input via plain input() if widgets unavailable
    if not use_widgets:
        user_input = input("Enter book title (partial or full): ").strip()
        if not user_input:
            print("No input.")
            return
        candidates = resolve_title_input(user_input)
        if not candidates:
            print("No matches found for:", user_input)
            return
        # If multiple candidates, show numbered list
        if len(candidates) > 1:
            print("Multiple matches found. Pick number:")
            for i, t in enumerate(candidates, 1):
                print(f"{i}. {t}")
            pick = input("Enter number (1-{}): ".format(len(candidates))).strip()
            try:
                p = int(pick) - 1
                chosen = candidates[p]
            except Exception:
                print("Invalid pick.")
                return
        else:
            chosen = candidates[0]
        print("Using title:", chosen)
        recommend_books_sparse(chosen, top_k=TOP_K)
        return

    # Widgets mode (notebook)
    txt = widgets.Text(placeholder='Type a book title (partial allowed)', description='Title:', layout=widgets.Layout(width='70%'))
    out = widgets.Output()
    dd = widgets.Dropdown(options=['(no matches yet)'], description='Matches:', layout=widgets.Layout(width='70%'))
    btn = widgets.Button(description='Recommend', button_style='success')

    def on_text_change(change):
        with out:
            clear_output(wait=True)
            val = change['new']
            if not val or len(val.strip()) < 2:
                dd.options = ['(type 2+ chars)']
                return
            cand = resolve_title_input(val, n_candidates=30)
            if not cand:
                dd.options = ['(no matches)']
            else:
                dd.options = cand[:200]

    def on_button_click(b):
        with out:
            clear_output(wait=True)
            chosen = dd.value
            if not chosen or chosen in ['(no matches)', '(type 2+ chars)']:
                print("Please select a valid match from the dropdown.")
                return
            print("Using title:", chosen)
            recommend_books_sparse(chosen, top_k=TOP_K)

    txt.observe(on_text_change, names='value')
    btn.on_click(on_button_click)
    display(txt, dd, btn, out)

# Run interactive helper
interactive_recommend()



# Cell D: Interactive text input + dropdown fallback. Works in notebook.
try:
    import ipywidgets as widgets
    from IPython.display import display, clear_output
    use_widgets = True
except Exception:
    use_widgets = False

def interactive_recommend():
    # Step 1: ask for input via plain input() if widgets unavailable
    if not use_widgets:
        user_input = input("Enter book title (partial or full): ").strip()
        if not user_input:
            print("No input.")
            return
        candidates = resolve_title_input(user_input)
        if not candidates:
            print("No matches found for:", user_input)
            return
        # If multiple candidates, show numbered list
        if len(candidates) > 1:
            print("Multiple matches found. Pick number:")
            for i, t in enumerate(candidates, 1):
                print(f"{i}. {t}")
            pick = input("Enter number (1-{}): ".format(len(candidates))).strip()
            try:
                p = int(pick) - 1
                chosen = candidates[p]
            except Exception:
                print("Invalid pick.")
                return
        else:
            chosen = candidates[0]
        print("Using title:", chosen)
        recommend_books_sparse(chosen, top_k=TOP_K)
        return

    # Widgets mode (notebook)
    txt = widgets.Text(placeholder='Type a book title (partial allowed)', description='Title:', layout=widgets.Layout(width='70%'))
    out = widgets.Output()
    dd = widgets.Dropdown(options=['(no matches yet)'], description='Matches:', layout=widgets.Layout(width='70%'))
    btn = widgets.Button(description='Recommend', button_style='success')

    def on_text_change(change):
        with out:
            clear_output(wait=True)
            val = change['new']
            if not val or len(val.strip()) < 2:
                dd.options = ['(type 2+ chars)']
                return
            cand = resolve_title_input(val, n_candidates=30)
            if not cand:
                dd.options = ['(no matches)']
            else:
                dd.options = cand[:200]

    def on_button_click(b):
        with out:
            clear_output(wait=True)
            chosen = dd.value
            if not chosen or chosen in ['(no matches)', '(type 2+ chars)']:
                print("Please select a valid match from the dropdown.")
                return
            print("Using title:", chosen)
            recommend_books_sparse(chosen, top_k=TOP_K)

    txt.observe(on_text_change, names='value')
    btn.on_click(on_button_click)
    display(txt, dd, btn, out)

# Run interactive helper
interactive_recommend()

# ===== Evaluate Item-Based Collaborative Filtering (Numerical Metrics - FIXED) =====
import numpy as np
from sklearn.metrics import ndcg_score

K = 5  # top-K recommendations

correct = 0
total = 0
precisions = []
recalls = []
ndcgs = []

for user, group in merged.groupby('user-id'):
    if len(group) < 2:
        continue  # skip users with only one rating
    test_row = group.sample(1)
    test_book = test_row['book-title'].values[0]
    train_books = group[group['book-title'] != test_book]['book-title'].tolist()

    # combine recommendations from user's other books
    recs = []
    for b in train_books:
        recs += recommend_books_sparse(b, top_k=K)
    recs = list(dict.fromkeys(recs))[:K]  # unique top-K

    if len(recs) == 0:
        continue

    # --- Compute Metrics ---
    hit = 1 if test_book in recs else 0
    correct += hit
    total += 1
    precisions.append(hit / K)
    recalls.append(hit)

    # --- NDCG@K (safe) ---
    y_true = np.zeros((1, len(recs)))
    if test_book in recs:
        idx = recs.index(test_book)
        y_true[0, idx] = 1
    y_score = np.arange(len(recs), 0, -1).reshape(1, -1)
    ndcgs.append(ndcg_score(y_true, y_score))

# --- Final Metrics ---
precision_at_k = np.mean(precisions) if precisions else 0
recall_at_k = np.mean(recalls) if recalls else 0
ndcg_at_k = np.mean(ndcgs) if ndcgs else 0

print("üìà Model Evaluation Metrics (Safe Version):")
print(f"Precision@{K}: {precision_at_k:.4f}")
print(f"Recall@{K} (Hit Rate): {recall_at_k:.4f}")
print(f"NDCG@{K}: {ndcg_at_k:.4f}")
print(f"Total users evaluated: {total}")
